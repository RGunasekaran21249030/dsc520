---
title: "Assignment_6.1_Housing_Data"
output:
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---


tinytex::install_tinytex()
tinytex:::is_tinytex() 
install.packages("pastecs")
install.packages("lm.beta");install.packages("Rmisc");install.packages("wavelets");install.packages("car")
install.packages("carData")
library(ggplot2); library(lm.beta); library(Rmisc); library(wavelets); library(car)
theme_set(theme_minimal())

```{r include = FALSE}
## Inline Code
library(readxl)

housing_df <- read_excel("week-6-housing.xlsx", sheet = "Sheet2")
show( housing_df)

```

a. Explain why you chose to remove data points from your ‘clean’ dataset.

Some of the data points like sale_instrument, sale_reason, sale warning, Site_type, etc does not provide any meaning to the real estate transactions or provide any insights to correlation and regressions.

b. Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.

```{r }
sqft_lm <- lm(housing_df$`Sale Price` ~ housing_df$sq_ft_lot)
```

```{r }
SalePrice_lm <-  lm(housing_df$`Sale Price` ~ housing_df$sq_ft_lot + housing_df$year_built + housing_df$square_feet_total_living + housing_df$bedrooms)

```
* Bedrooms - Sale Price increases with more number of bedrooms in the house
* Year Built - Sale Price may decrease with older the year built of the real estate propoerty
* Total Square Foot - Sale Price may increase with more total square foot living area

c. Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r }

summary(sqft_lm)

plot(sqft_lm, which=1, col=c("blue")) # Residuals vs Fitted Plot

plot(sqft_lm, which=2, col=c("red"))  # Q-Q Plot

plot(sqft_lm, which=3, col=c("blue"))  # Scale-Location Plot

plot(sqft_lm, which=5, col=c("blue"))  # Residuals vs Leverage


```

* R-squared: 0.01435
* Adjusted R-squared: 0.01428

The R-squared value is a measure of variability in the outcome obtained by the predictors. Square Foot of the Lot accounts for 1.435% of the variation in Sale Price.

The Adjusted R-Squared generalizes the outcome, whose value is close to R-squared value. the Whole model thus provides just 0.01% variance, proving the cross-validity is extremely good.

By including additional Predictor Variables in the model would result into the following outcome:
\
```{r }

summary(SalePrice_lm)

```

* R-squared:  0.2202
* Adjusted R-squared:  0.22

By including other predictors year_built, square_feet_total_living, bedrooms, the value for R-squared increases to 0.2202 or 22.2% of the variance in the Sale Price. Therefore these predictor models contribute to the most variance in the Sale price of the property. 

d. Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?


Primarily if the Square Foot of Lot increases by one standard deviation, Sale Price increases by 0.03842553 standard deviation.

Year Built of Lot increases by one standard deviation, Sale Price increases by 0.11928935 standard deviation.

Square Feet of total living increases by one standard deviation, Sale Price increases by 0.42036390 standard deviation.


e. Calculate the confidence intervals for the parameters in your model and explain what the results indicate.

```{r }

confint(SalePrice_lm)

```

The two best predictors - Number of bedrooms and Square Feet of total living area have confidence intervals that are close, indicating that the estimates for current model are likely to be representative of the true population values.


f. Assess the improvement of the new model compared to your original model (simple regression model) by testing whether this change is significant by performing an analysis of variance.

```{r }

anova(sqft_lm, SalePrice_lm)

```


g. Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

```{r }

SalePrice_DataFrame <- as.data.frame(resid(SalePrice_lm))
SalePrice_DataFrame$residuals <- resid(SalePrice_lm)
SalePrice_DataFrame$standardized.residuals <- rstandard(SalePrice_lm)
SalePrice_DataFrame$studentized.residuals <- rstudent(SalePrice_lm)
SalePrice_DataFrame$cooks.distance <- cooks.distance(SalePrice_lm)
SalePrice_DataFrame$dfbeta <- dfbeta(SalePrice_lm)
SalePrice_DataFrame$dffit <- dffits(SalePrice_lm)
SalePrice_DataFrame$leverage <- hatvalues(SalePrice_lm)
SalePrice_DataFrame$covariance.ratios <- covratio(SalePrice_lm)
head(SalePrice_DataFrame, 10)

```

h. Calculate the standardized residuals using the appropriate command, specifying those that are +-2, storing the results of large residuals in a variable you create.

```{r }

SalePrice_DataFrame$large.residuals <- SalePrice_DataFrame$standardized.residuals > 2 | SalePrice_DataFrame$standardized.residuals < 2

```

i. Use the appropriate function to show the sum of large residuals.

```{r }

sum(SalePrice_DataFrame$large.residuals)

```

j. Which specific variables have large residuals (only cases that evaluate as TRUE)?

```{r }

head(SalePrice_DataFrame$large.residuals, 50)

```

k.Investigate further by calculating the leverage, cooks distance, and covariance rations. Comment on all cases that are problematics.

```{r }

head(SalePrice_DataFrame[SalePrice_DataFrame$large.residuals, c("cooks.distance", "leverage", "covariance.ratios")], 10)

```
Zero cases are problematic

l. Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.
```{r }

library(carData)
library(car)

dwt(SalePrice_lm)

```

Relating to the conservacy rule, if the value < 1 and > 3, then that should raise the concern. In relative terms,if the values are much better if they are around 2. The D-W Statistic Value is 0.5581258. The value is less than 1. So the assumption of independence is not met.

m. Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

```{r }

## assumption of no multicollinearity

vif(SalePrice_lm)

## calculate tolerance = 1 / VIF

1/vif(SalePrice_lm)

## Average VIF

mean(vif(SalePrice_lm))

```

The largest VIF is not greater than 10 so there is no cause of concern.
Average VIF is not substancially greater 1 than so the regression may be unbiased.
Tolerance is not below 0.1 so there is no serious problem.
Tolerance is not below 0.2 either so there is no potential problem.

So the assumption of no multicollinearity is met.


n. Visually check the assumptions related to the residuals using the plot() and hist() functions. Summarize what each graph is informing you of and if any anomalies are present.

```{r }

## Standardized Residual (y-axis)
## Predicted (Fitted) Value (x-axis)

plot(dffits(SalePrice_lm), rstandard(SalePrice_lm), xlab = "Predicted (Fitted) Value", ylab = "Standardized Residual", main="Standardized Residual Vs Predicted (Fitted) Value")

```

1. Plot does not show the dots funnels out so there is not heteroscedasticity in the data.
2. plot does not show the curvilinear trend so the data have not violated assumption of linearity.
3. Plot does not show the non-linear relationship and also is not heteroscedasticity in the data.
4. Plot does show the random dispersed of the dots around zero. So the assumptions of linearity, randomness and heteroscedasticity have been met.

```{r }
## qqplot()
## Predicted (Fitted) Value (x-axis)

qqPlot(dffits(SalePrice_lm))

```

Q-Q Plot produced by the plot() function shows few data points deviate from the normality. Otherwise assumptions of linearity has been met. The straight line in this plot plot represents the normal distribution, and the points represents the observed residuals.

Below are the two anomalies observed:

1. Data Point 4649
2. Data Point 8377

```{r }

## hist()
## Studentized Residual (x-axis)

hist(SalePrice_DataFrame$studentized.residuals, main = "Histogram of Studentized Residual")

```

The histogram should looks like normal distribution (a bell-shaped curve). In this case, the Housing Data is roughly normal and slightly skewed to the right.

o.Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

Yes. Regression model is unbiased whose Average VIF (1.480344) is not extremely greater than 1.

The difference between Adjusted R-squared and R-squared = 0.01%. This means that model derived from the population rather than given sample in the data set would account for approximately 0.01% less variance in the outcome.

This also indicates that the sample for the current model is likely to be representative of the true population values.

The model appears to be very good both for the sample and also the overall population. We can conclude that in our  Square Feet Living Area and sample Year Built are equally important in addition to the Square Foot of the Lot in predicting the Sale Price of the real estate. Number of Bedrooms is equally important but less important than other three predictors. The assumptions have also been fairly met.

To conclude, this model would fit into any housing data across any demographic or geography. 


## References
1.  Gallo, Amy. (2015). A Refresher on Regression Analysis.
2.  Martin, Rose. (2018). Using Linear Regression for Predictive Modeling in R.
3.  Statistics How To. Regression Analysis: Step by Step Articles, Videos, Simples Definitions.
 
